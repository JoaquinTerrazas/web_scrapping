{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9148f1",
   "metadata": {},
   "source": [
    "# Instalamos e importamos librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6f292f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: textblob in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: seaborn in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.8.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: click in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from nltk>=3.9->textblob) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from nltk>=3.9->textblob) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from nltk>=3.9->textblob) (2025.8.29)\n",
      "Requirement already satisfied: tqdm in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\anaconda3\\envs\\yelp_scraper\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install praw pandas textblob matplotlib seaborn\n",
    "import praw\n",
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cd7282",
   "metadata": {},
   "source": [
    "# Guardar resultados en la carpeta output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96132084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando carpeta existente: ../output\n",
      "=== CONFIGURACIÓN COMPLETADA ===\n",
      "Directorio de trabajo: c:\\Users\\Usuario\\Desktop\\CICLOWAA8\\data science\\web_scrapping\\code\n",
      "Carpeta de salida: ../output\n"
     ]
    }
   ],
   "source": [
    "output_dir = '../output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Carpeta '{output_dir}' creada\")\n",
    "else:\n",
    "    print(f\"Usando carpeta existente: {output_dir}\")\n",
    "\n",
    "print(\"=== CONFIGURACIÓN COMPLETADA ===\")\n",
    "print(f\"Directorio de trabajo: {os.getcwd()}\")\n",
    "print(f\"Carpeta de salida: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d9bdc4",
   "metadata": {},
   "source": [
    "# Configuramos credenciales de Reddit y probamos conexión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4ee05d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDDIT_CONFIG = {\n",
    "    'client_id': 'gj_-APZ-9Jb84203m99zIg',          \n",
    "    'client_secret': 'I2R1lhOwO_Ofy8tpNTjx1H3gpBA-Zg',   \n",
    "    'username': 'Wonderful-Band-2611',          \n",
    "    'password': 'pancho11',           \n",
    "    'user_agent': 'Python:PoliticalSentimentAnalyzer:v1.0 (by /u/Wonderful-Band-2611)'  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8dfaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PARTE 1: REDDIT API SETUP ===\n",
      "Conexión exitosa como: Wonderful-Band-2611\n",
      "Karma de comentarios: 0\n",
      "Karma de posts: 1\n",
      "Acceso a r/politics: 8,883,094 suscriptores\n",
      "API Setup completado correctamente\n"
     ]
    }
   ],
   "source": [
    "def initialize_reddit():\n",
    "    \"\"\"API Connection (PRAW) - Conectar usando credenciales\"\"\"\n",
    "    try:\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=REDDIT_CONFIG['client_id'],\n",
    "            client_secret=REDDIT_CONFIG['client_secret'],\n",
    "            user_agent=REDDIT_CONFIG['user_agent'],\n",
    "            username=REDDIT_CONFIG['username'],\n",
    "            password=REDDIT_CONFIG['password']\n",
    "        )\n",
    "        user = reddit.user.me()\n",
    "        print(f\"Conectado como: {user}\")\n",
    "        return reddit\n",
    "    except Exception as e:\n",
    "        print(f\"Error de conexión: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_reddit_connection():\n",
    "    \"\"\"Función para probar la conexión antes de empezar\"\"\"\n",
    "    try:\n",
    "        reddit = praw.Reddit(**REDDIT_CONFIG)\n",
    "        user = reddit.user.me()\n",
    "        print(f\"Conexión exitosa como: {user}\")\n",
    "        print(f\"Karma de comentarios: {user.comment_karma}\")\n",
    "        print(f\"Karma de posts: {user.link_karma}\")\n",
    "        \n",
    "        test_subreddit = reddit.subreddit('politics')\n",
    "        print(f\"Acceso a r/politics: {test_subreddit.subscribers:,} suscriptores\")\n",
    "        return reddit\n",
    "    except Exception as e:\n",
    "        print(f\"Error de conexión: {e}\")\n",
    "        return None\n",
    "\n",
    "# EJECUCIÓN PARTE 1\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== PARTE 1: REDDIT API SETUP ===\")\n",
    "    reddit_instance = test_reddit_connection()\n",
    "    \n",
    "    if reddit_instance:\n",
    "        print(\"API Setup completado correctamente\")\n",
    "    else:\n",
    "        print(\"Revisa tu configuración antes de continuar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9270a5",
   "metadata": {},
   "source": [
    "# PARTE 2A: COLLECT POSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "435b5f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PARTE 2A: COLLECT POSTS (3pts) ===\n",
      "Conectado como: Wonderful-Band-2611\n",
      "RECOLECTANDO POSTS...\n",
      "Procesando r/politics...\n",
      "r/politics: 20 posts recolectados\n",
      "Procesando r/PoliticalDiscussion...\n",
      "r/PoliticalDiscussion: 20 posts recolectados\n",
      "Procesando r/worldnews...\n",
      "r/worldnews: 20 posts recolectados\n",
      "Total posts recolectados: 60\n",
      "Posts guardados en output/\n"
     ]
    }
   ],
   "source": [
    "def collect_posts(reddit):\n",
    "    \"\"\"\n",
    "    Collect Posts from Subreddits (3pts)\n",
    "    - Target: r/politics, r/PoliticalDiscussion, r/worldnews\n",
    "    - Task: 20 \"hot\" posts per subreddit\n",
    "    - Extract: title, score, num_comments, id, url\n",
    "    \"\"\"\n",
    "    target_subreddits = ['politics', 'PoliticalDiscussion', 'worldnews']\n",
    "    posts_data = []\n",
    "    \n",
    "    print(\"RECOLECTANDO POSTS...\")\n",
    "    for subreddit_name in target_subreddits:\n",
    "        try:\n",
    "            print(f\"Procesando r/{subreddit_name}...\")\n",
    "            subreddit = reddit.subreddit(subreddit_name)\n",
    "            \n",
    "            for post in subreddit.hot(limit=20):\n",
    "                posts_data.append({\n",
    "                    'title': post.title,           # título según tarea\n",
    "                    'score': post.score,           # upvotes según tarea\n",
    "                    'num_comments': post.num_comments,  # num_comments según tarea\n",
    "                    'id': post.id,                 # unique identifier según tarea\n",
    "                    'url': post.url                # url según tarea\n",
    "                })\n",
    "                time.sleep(0.1)\n",
    "            \n",
    "            print(f\"r/{subreddit_name}: 20 posts recolectados\")\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en r/{subreddit_name}: {e}\")\n",
    "    \n",
    "    posts_df = pd.DataFrame(posts_data)\n",
    "    print(f\"Total posts recolectados: {len(posts_df)}\")\n",
    "    return posts_df\n",
    "\n",
    "# EJECUCIÓN PARTE 2A\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== PARTE 2A: COLLECT POSTS (3pts) ===\")\n",
    "    reddit = initialize_reddit()\n",
    "    if reddit:\n",
    "        posts_df = collect_posts(reddit)\n",
    "        \n",
    "        # Guardar posts temporalmente para Parte 2B\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        posts_df.to_csv(os.path.join(output_dir, f'posts_data_{timestamp}.csv'), index=False)\n",
    "        posts_df.to_csv(os.path.join(output_dir, 'posts_data.csv'), index=False)\n",
    "        print(\"Posts guardados en output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accf1a95",
   "metadata": {},
   "source": [
    "# PARTE 2B: COLLECT COMMENTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93c96f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PARTE 2B: COLLECT COMMENTS (3pts) ===\n",
      "Conectado como: Wonderful-Band-2611\n",
      "\n",
      "RECOLECTANDO COMENTARIOS...\n",
      "Post: To defend against Russian tanks, Finland...\n",
      "Post: Donald Trump Declares D.C. a 'Crime Free...\n",
      "Post: Donald Trump's 50-Day Deadline to Russia...\n",
      "Post: Maduro warns of \"bloody threat\" as Trump...\n",
      "Post: Trump, Gabbard fired top CIA Russia expe...\n",
      "Post: Donald Trump to make televised announcem...\n",
      "Post: Indian PM meets Putin and urges him to s...\n",
      "Post: Alarm after FBI arrests US army veteran ...\n",
      "Post: Kremlin denies Trump, Putin ever agreed ...\n",
      "Post: Trump illegally sent National Guard to L...\n",
      "Post: France orders hospitals war-ready by Mar...\n",
      "Post: Nestle abruptly removes CEO Freixe over ...\n",
      "Post: Ukraine’s Flamingo missile reportedly ma...\n",
      "Post: Americans Lose Faith That Hard Work Lead...\n",
      "Post: Bizarre Video Shows Mystery Objects Thro...\n",
      "Total comentarios recolectados: 75\n",
      "Comentarios guardados en output/\n"
     ]
    }
   ],
   "source": [
    "def collect_comments(reddit, posts_df):\n",
    "    \"\"\"\n",
    "    Collect Comments (3pts)\n",
    "    - Task: For subset of most relevant posts collect 5 comments per post\n",
    "    - Extract: body, score, post_id\n",
    "    \"\"\"\n",
    "    print(\"\\nRECOLECTANDO COMENTARIOS...\")\n",
    "    \n",
    "    # Seleccionar posts más relevantes por score\n",
    "    most_relevant_posts = posts_df.nlargest(15, 'score')\n",
    "    comments_data = []\n",
    "    \n",
    "    for _, post_row in most_relevant_posts.iterrows():\n",
    "        try:\n",
    "            print(f\"Post: {post_row['title'][:40]}...\")\n",
    "            submission = reddit.submission(id=post_row['id'])\n",
    "            submission.comments.replace_more(limit=0)\n",
    "            \n",
    "            # Recolectar exactamente 5 comentarios como pide la tarea\n",
    "            comment_count = 0\n",
    "            for comment in submission.comments:\n",
    "                if (hasattr(comment, 'body') and \n",
    "                    comment.body not in ['[deleted]', '[removed]'] and \n",
    "                    comment_count < 5):\n",
    "                    \n",
    "                    comments_data.append({\n",
    "                        'body': comment.body,          # comment text según tarea\n",
    "                        'score': comment.score,        # upvotes on comment según tarea\n",
    "                        'post_id': post_row['id']      # link back to original post según tarea\n",
    "                    })\n",
    "                    comment_count += 1\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en post {post_row['id']}: {str(e)[:50]}...\")\n",
    "    \n",
    "    comments_df = pd.DataFrame(comments_data)\n",
    "    print(f\"Total comentarios recolectados: {len(comments_df)}\")\n",
    "    return comments_df\n",
    "\n",
    "# EJECUCIÓN PARTE 2B\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== PARTE 2B: COLLECT COMMENTS (3pts) ===\")\n",
    "    \n",
    "    # Cargar posts de Parte 2A\n",
    "    try:\n",
    "        posts_df = pd.read_csv(os.path.join(output_dir, 'posts_data.csv'))\n",
    "        reddit = initialize_reddit()\n",
    "        if reddit:\n",
    "            comments_df = collect_comments(reddit, posts_df)\n",
    "            \n",
    "            # Guardar comentarios\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            comments_df.to_csv(os.path.join(output_dir, f'comments_data_{timestamp}.csv'), index=False)\n",
    "            comments_df.to_csv(os.path.join(output_dir, 'comments_data.csv'), index=False)\n",
    "            print(\"Comentarios guardados en output/\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Ejecuta primero la Parte 2A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc6c9fe",
   "metadata": {},
   "source": [
    "# PARTE 2C: STORAGE & LINKING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5120bab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PARTE 2C: STORAGE & LINKING (2pts) ===\n",
      "\n",
      "GUARDANDO Y VINCULANDO DATOS...\n",
      "DATOS GUARDADOS Y VINCULADOS:\n",
      "Posts guardados: 60\n",
      "Comentarios guardados: 75\n",
      "Posts con comentarios: 15\n",
      "Total vinculaciones: 75\n",
      "\n",
      "EJEMPLO DE VINCULACIÓN:\n",
      "Post ID: 1n5tdc8\n",
      "Título: To defend against Russian tanks, Finland and Polan...\n",
      "Comentarios vinculados: 5\n",
      "Vinculación completada\n"
     ]
    }
   ],
   "source": [
    "def store_and_link_data(posts_df, comments_df):\n",
    "    \"\"\"\n",
    "    Storage: Store data linking each comment to its parent post (2pts)\n",
    "    \"\"\"\n",
    "    print(\"\\nGUARDANDO Y VINCULANDO DATOS...\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Guardar posts con timestamp y archivo fijo\n",
    "    posts_df.to_csv(os.path.join(output_dir, f'posts_data_{timestamp}.csv'), index=False)\n",
    "    posts_df.to_csv(os.path.join(output_dir, 'posts_data.csv'), index=False)\n",
    "    \n",
    "    # Guardar comentarios (ya contienen post_id para vinculación)\n",
    "    comments_df.to_csv(os.path.join(output_dir, f'comments_data_{timestamp}.csv'), index=False)\n",
    "    comments_df.to_csv(os.path.join(output_dir, 'comments_data.csv'), index=False)\n",
    "    \n",
    "    # Verificar vinculación según tarea\n",
    "    posts_with_comments = comments_df['post_id'].nunique()\n",
    "    total_linkages = len(comments_df)\n",
    "    \n",
    "    print(\"DATOS GUARDADOS Y VINCULADOS:\")\n",
    "    print(f\"Posts guardados: {len(posts_df)}\")\n",
    "    print(f\"Comentarios guardados: {len(comments_df)}\")\n",
    "    print(f\"Posts con comentarios: {posts_with_comments}\")\n",
    "    print(f\"Total vinculaciones: {total_linkages}\")\n",
    "    \n",
    "    # Ejemplo de vinculación\n",
    "    if not comments_df.empty:\n",
    "        sample_post_id = comments_df.iloc[0]['post_id']\n",
    "        sample_post = posts_df[posts_df['id'] == sample_post_id].iloc[0]\n",
    "        sample_comments = comments_df[comments_df['post_id'] == sample_post_id]\n",
    "        \n",
    "        print(f\"\\nEJEMPLO DE VINCULACIÓN:\")\n",
    "        print(f\"Post ID: {sample_post_id}\")\n",
    "        print(f\"Título: {sample_post['title'][:50]}...\")\n",
    "        print(f\"Comentarios vinculados: {len(sample_comments)}\")\n",
    "\n",
    "# EJECUCIÓN PARTE 2C\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== PARTE 2C: STORAGE & LINKING (2pts) ===\")\n",
    "    \n",
    "    try:\n",
    "        posts_df = pd.read_csv(os.path.join(output_dir, 'posts_data.csv'))\n",
    "        comments_df = pd.read_csv(os.path.join(output_dir, 'comments_data.csv'))\n",
    "        \n",
    "        store_and_link_data(posts_df, comments_df)\n",
    "        print(\"Vinculación completada\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Ejecuta primero las Partes 2A y 2B\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yelp_scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
